# -*- coding: utf-8 -*-
"""2번째 미니프로젝트(넷플릭스 이탈률).ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/13pUgoAfrhIQwK2YigC6c7FOqoEXaMWlL
"""

import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt

import warnings
warnings.filterwarnings('ignore')

"""# 데이터 로드 하기"""

df= pd.read_csv('/content/netflix_customer_churn.csv')

#SEED 설정
SEED=42

df.head()

df.info()

"""## object형
customer_id, gender, subscription_type, region, device, payment_method, favorit_genre

## 수치형
age, watch_hours, last_login_days, monthly_fee, number_of_profiles, avg_watch_time_per_day
"""

df.isnull().sum() # 좋아 다행히 결측치는 없네

#target Distribution
sns.countplot(x='churned', data=df)
plt.title("Target:Churn Distribution")

# 나간 사람: 2515명 / 안나간 사람: 2485 비율이 거의 비슷하다

churn_counts = df['churned'].value_counts()
print("Churn Distribution:")
print(churn_counts)

"""# 데이터 이해하기"""

# EDA: Age vs Churn
sns.histplot(data=df, x='age', hue='churned', kde=True, bins=30)
plt.title("Age Distributeion by Churn")

# EDA: watch_hours vs Churn # watch_hours가 낮으면 낮을수록 이탈각이 높음!
sns.histplot(data=df, x='watch_hours', hue='churned', kde=True, bins=30)
plt.title("watch_hours Distributeion by Churn")

# EDA: last_login_days vs Churn # 마지막 로그인이 30일 이상이면 이탈각 증가
sns.histplot(data=df, x='last_login_days', hue='churned', kde=True, bins=30)
plt.title("last_login_days Distributeion by Churn")

# EDA: monthly_fee vs Churn  # 9달러 내는 사람들 사이에서 해지가 제일 높네 이상하게14나 18달러는 유지하는 사람이 많고
sns.histplot(data=df, x='monthly_fee', hue='churned', kde=True, bins=30)
plt.title("monthly_fee Distributeion by Churn")

# EDA: number_of_profiles vs Churn  # 흠... 3.5개 이상의 계정(?)을 가지면 이탈률이 급격히 낮아짐
sns.histplot(data=df, x='number_of_profiles', hue='churned', kde=True, bins=30)
plt.title("number_of_profiles Distributeion by Churn")

# EDA: gender vs Churn  # 흠... other는 누구냐? transgender? 여자 이탈률이 남자보다 높네...
sns.histplot(data=df, x='gender', hue='churned', kde=True, bins=30)
plt.title("gender Distributeion by Churn")

# EDA: subscription_type vs Churn  # 확실해 Basic에서 많은 이탈이 나타나네
sns.histplot(data=df, x='subscription_type', hue='churned', kde=True, bins=30)
plt.title("subscription_type Distributeion by Churn")

# EDA: region vs Churn  # 유럽과 남아메리카에서 이탈률이 심하군 아시아도..
sns.histplot(data=df, x='region', hue='churned', kde=True, bins=30)
plt.title("region Distributeion by Churn")

# EDA: device vs Churn  # 노트북과 모바일에서 많은 이탈률이 나오는군..
sns.histplot(data=df, x='device', hue='churned', kde=True, bins=30)
plt.title("device Distributeion by Churn")

# EDA: payment_method vs Churn  # 오 기프트 카드하고 암호카페에서 엄청나게 이탈률이 나오네
sns.histplot(data=df, x='payment_method', hue='churned', kde=True, bins=30)
plt.title("payment_method Distributeion by Churn")

# EDA: favorite_genre vs Churn  # 호오 액션, 드라마, 호러, 다큐에서 많은 이탈률이 나오네
sns.histplot(data=df, x='favorite_genre', hue='churned', kde=True, bins=30)
plt.title("favorite_genre Distributeion by Churn")

"""# 전처리"""

# preprocessing
df.drop(columns=['customer_id'], inplace=True)# 필요없는 컬럼 삭제

"""# Feature 엔진니어링"""

# # Feature Engineering
# df['watch_hours_per_profile']=df['watch_hours']/df['number_of_profiles']
# df['login_recent']=(df['last_login_days']<10).astype(int)

# # Categorical Encoding
# df= pd.get_dummies(df, drop_first=True)

from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score

"""# 검증데이터 나누기"""

# split Data
X = df.drop('churned', axis=1)
y = df['churned']
X_train,X_test,y_train,y_test = train_test_split(X,y, test_size=0.2, stratify=y, random_state=89)
X_train.shape,X_test.shape,y_train.shape,y_test.shape

"""# 인코딩(원핫인코딩)"""

from sklearn.preprocessing import OneHotEncoder

categorical_features = X_train.select_dtypes(include=['object']).columns
encoder = OneHotEncoder(handle_unknown='ignore')
X_train_encoded= encoder.fit_transform(X_train[categorical_features])
X_test_encoded=encoder.transform(X_test[categorical_features])
categorical_features

#X_train,X_test에 one hot encoding 적용
train_enc = pd.DataFrame(X_train_encoded.toarray(), columns=encoder.get_feature_names_out(categorical_features))
test_enc = pd.DataFrame(X_test_encoded.toarray(), columns=encoder.get_feature_names_out(categorical_features))
train_enc

test_enc

# encoding 데이터와 수치형 데이터 병합
train_df= X_train.select_dtypes(include=['int64','float64']).join(train_enc)
test_df= X_test.select_dtypes(include=['int64','float64']).join(test_enc)
train_df

"""# 스케일링"""

from sklearn.preprocessing import StandardScaler
scaler=StandardScaler()
train_df=pd.DataFrame(scaler.fit_transform(train_df), columns=train_df.columns)
test_df = pd.DataFrame(scaler.transform(test_df), columns=test_df.columns)
train_df

# Scaling
# scaler = StandardScaler()
# X_train_scaled= scaler.fit_transform(X_train)
# X_test_scaled=scaler.transform(X_test)

# #model- Random Forest
# model= RandomForestClassifier(random_state=89)
# model.fit(X_train_scaled, y_train)
# y_pred=model.predict(X_test_scaled)

"""# Training RandomForest"""

# Initialize and train the RandomForestClassifier model
model = RandomForestClassifier(random_state=SEED)
model.fit(train_df, y_train)

# Make predictions on the test data
y_pred = model.predict(test_df)

# Evaluate the model
print(classification_report(y_test, y_pred))
print("ROC-AUC:", roc_auc_score(y_test, model.predict_proba(test_df)[:, 1]))
print(f'훈련용 평가지표: {model.score(train_df, y_train)} / 테스트용 평가지표: {model.score(test_df, y_test)}')





# rf=RandomForestClassifier(random_state=89)
# rf.fit(X_train,y_train)
# pred=rf.predict(X_train)
# pred

# # Evaluation
# print(classification_report(y_test, y_pred))
# print("ROC-AUC:", roc_auc_score(y_test,model.predict_proba(X_test_scaled)[:,1]))

# Feature Importance
importances = pd.Series(model.feature_importances_, index=train_df.columns)
importances.nlargest(10).plot(kind='barh')
plt.title("Top Feature Importances")
plt.show()

"""# Confusion Matrix"""

from sklearn.metrics import confusion_matrix
import seaborn as sns

cm = confusion_matrix(y_test, y_pred)
sns.heatmap(cm, annot=True, fmt ='d', cmap= 'Blues')
plt.xlabel("predicted")
plt.ylabel('actual')
plt.title('confusion matrix')



