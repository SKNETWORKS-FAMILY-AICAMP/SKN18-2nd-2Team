# ëª¨ë¸ë³„ ì„±ëŠ¥ í™•ì¸ ë° ìµœì í™”

import lightgbm as lgb
import optuna
from catboost import CatBoostClassifier
from sklearn.ensemble import (
    AdaBoostClassifier,
    ExtraTreesClassifier,
    GradientBoostingClassifier,
    HistGradientBoostingClassifier,
    RandomForestClassifier,
)
from sklearn.linear_model import LogisticRegression, RidgeClassifier
from sklearn.model_selection import GridSearchCV, KFold, cross_val_score
from sklearn.neighbors import KNeighborsClassifier
from sklearn.neural_network import MLPClassifier
from sklearn.pipeline import Pipeline
from sklearn.svm import SVC
import xgboost
from utilities.utils import reset_seeds
from xgboost import XGBClassifier

import pickle

# ë² ì´ìŠ¤ ëª¨ë¸ ìƒì„±

MODELS = {
    "LogisticRegression": LogisticRegression(max_iter=1000, random_state=42),
    "RandomForest": RandomForestClassifier(random_state=42),
    "XGBoost": xgboost.XGBClassifier(
        random_state=42, eval_metric="logloss"
    ),
    "LightGBM": lgb.LGBMClassifier(random_state=42),
    "CatBoost": CatBoostClassifier(random_state=42, verbose=0),
    "SVC": SVC(probability=True, random_state=42),
    "ExtraTrees": ExtraTreesClassifier(random_state=42),
    "AdaBoost": AdaBoostClassifier(random_state=42),
    "HistGradientBoosting": HistGradientBoostingClassifier(random_state=42),
    "GradientBoosting": GradientBoostingClassifier(random_state=42),
    "KNeighbors": KNeighborsClassifier(),
    "RidgeClassifier": RidgeClassifier(random_state=42),
    "MLPClassifier": MLPClassifier(random_state=42, max_iter=1000),
}

# í•˜ì´í¼íŒŒë¼ë¯¸í„° ë²”ìœ„ì§€ì •
def __pre_hpo(trial, model_name, X, y):
    model = MODELS[model_name]
    # ì£¼ì˜ ì§„ì§œ ë§¤ìš° ì•„ì£¼ ì˜¤ë˜ ê±¸ë¦¼
    # ëª¨ë¸ì— ë”°ë¼ í•˜ì´í¼íŒŒë¼ë¯¸í„° ë²”ìœ„ ì„¤ì •
    if model_name == "XGBoost":
        params = {
            "n_estimators": trial.suggest_int("n_estimators", 40, 200),
            "max_depth": trial.suggest_int("max_depth", 2, 6),
            "learning_rate": trial.suggest_float("learning_rate", 0.01, 0.3),
            "subsample": trial.suggest_float("subsample", 0.6, 1.0),
            "colsample_bytree": trial.suggest_float("colsample_bytree", 0.6, 1.0),
            "gamma": trial.suggest_float("gamma", 0.1, 5.0),
            "min_child_weight": trial.suggest_int("min_child_weight", 1, 10),
        }
        model.set_params(**params)

    elif model_name in ["RandomForest", "ExtraTrees"]:
        params = {
            "n_estimators": trial.suggest_int("n_estimators", 100, 500),
            "max_depth": trial.suggest_int("max_depth", 5, 20),
            "min_samples_split": trial.suggest_int("min_samples_split", 2, 10),
            "min_samples_leaf": trial.suggest_int("min_samples_leaf", 1, 4),
        }
        model.set_params(**params)

    elif model_name == "LightGBM":
        params = {
            "learning_rate": trial.suggest_float("learning_rate", 0.01, 0.3),
            "num_leaves": trial.suggest_int("num_leaves", 20, 150),
        }
        model.set_params(**params)

    elif model_name == "CatBoost":
        params = {
            "iterations": trial.suggest_int(
                "iterations", 50, 300
            ),  # ë” ì ì€ ë²”ìœ„ë¡œ ì¤„ì„
            "learning_rate": trial.suggest_float(
                "learning_rate", 0.01, 0.3
            ),  # learning_rate ë²”ìœ„ ì¡°ì •
        }
        model.set_params(**params)

    elif model_name == "LogisticRegression":
        params = {"C": trial.suggest_float("C", 0.001, 10)}
        model.set_params(**params)

    elif model_name == "SVC":
        params = {
            "C": trial.suggest_float("C", 0.1, 10),
            "gamma": trial.suggest_float("gamma", 0.001, 1.0),
        }
        model.set_params(**params)

    elif model_name == "AdaBoost":
        params = {
            "n_estimators": trial.suggest_int("n_estimators", 50, 500),
            "learning_rate": trial.suggest_float("learning_rate", 0.01, 1.0),
        }
        model.set_params(**params)

    elif model_name == "HistGradientBoosting":
        params = {
            "learning_rate": trial.suggest_float("learning_rate", 0.01, 0.3),
            "max_depth": trial.suggest_int("max_depth", 3, 15),
        }
        model.set_params(**params)

    elif model_name == "GradientBoosting":
        params = {
            "n_estimators": trial.suggest_int("n_estimators", 100, 500),
            "learning_rate": trial.suggest_float("learning_rate", 0.01, 0.1),
            "max_depth": trial.suggest_int("max_depth", 3, 5),
            "subsample": trial.suggest_float("subsample", 0.6, 1.0),
        }
        model.set_params(**params)

    elif model_name == "KNeighbors":
        params = {
            "n_neighbors": trial.suggest_int("n_neighbors", 3, 15),
        }
        model.set_params(**params)

    elif model_name == "RidgeClassifier":
        params = {"alpha": trial.suggest_float("alpha", 0.1, 10.0)}
        model.set_params(**params)

    elif model_name == "MLPClassifier":
        params = {
            "alpha": trial.suggest_float("alpha", 0.0001, 0.01),
        }
        model.set_params(**params)

    pipeline = Pipeline(steps=[("model", model)])

    # êµì°¨ ê²€ì¦ì„ í†µí•œ ëª¨ë¸ ì„±ëŠ¥ í‰ê°€
    cv = KFold(n_splits=10, shuffle=True, random_state=42)
    scores = cross_val_score(pipeline, X, y, cv=cv, scoring="accuracy", n_jobs=-1)

    return scores.mean()

# í•˜ì´í¼ íŒŒë¼ë¯¸í„° ìµœì í™”
def __run_prehpo(X_train_selected, y_train_smote):
    # í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™” ìˆ˜í–‰
    best_params = {}
    for model_name in MODELS.keys():
        try:
            study = optuna.create_study(direction="maximize")
            study.optimize(
                lambda trial: __pre_hpo(trial, model_name, X_train_selected, y_train_smote),
                n_trials=50,
                n_jobs=-1,
            )  # n_trialsë¡œ ì¡°ì ˆ
            best_params[model_name] = study.best_params
            print(f"Best params for {model_name}: {best_params[model_name]}")
        except Exception as e:
            print(f"âš ï¸  Error optimizing {model_name}: {e}")
            best_params[model_name] = None
    
    return best_params

# gridsearchcvë¡œ ì •ë°€í•˜ê²Œ ìµœì í™”
def __run_hpo(best_params, X_selected, y_smote):
    """
    ì£¼ì–´ì§„ ëª¨ë¸ë“¤ì— ëŒ€í•´ GridSearchCVë¥¼ ì‚¬ìš©í•˜ì—¬ ìµœì ì˜ í•˜ì´í¼íŒŒë¼ë¯¸í„°ë¥¼ ì°¾ëŠ” í•¨ìˆ˜.

    Parameters:
    models (dict): ëª¨ë¸ ì´ë¦„ê³¼ ëª¨ë¸ ê°ì²´ë¡œ êµ¬ì„±ëœ ë”•ì…”ë„ˆë¦¬.
    best_params (dict): ê° ëª¨ë¸ì— ëŒ€í•œ Optunaë¡œ ì°¾ì€ ìµœì  íŒŒë¼ë¯¸í„°.
    X_selected (DataFrame): í•™ìŠµìš© í”¼ì²˜ ë°ì´í„°.
    y_smote (Series): í•™ìŠµìš© íƒ€ê²Ÿ ë°ì´í„°.

    Returns:
    final_models (dict): ê° ëª¨ë¸ì˜ ìµœì  í•˜ì´í¼íŒŒë¼ë¯¸í„°ë¡œ í•™ìŠµëœ ëª¨ë¸.
    """
    final_models = {}
    
    # best_paramsê°€ Noneì¸ ê²½ìš° ì²˜ë¦¬
    if best_params is None:
        print("âš ï¸  best_paramsê°€ Noneì…ë‹ˆë‹¤. GridSearchCVë¥¼ ê±´ë„ˆëœë‹ˆë‹¤.")
        return final_models

    for model_name in MODELS.keys():
        # best_paramsì— í•´ë‹¹ ëª¨ë¸ì´ ì—†ê±°ë‚˜ Noneì¸ ê²½ìš° ê±´ë„ˆë›°ê¸°
        if model_name not in best_params or best_params[model_name] is None:
            print(f"âš ï¸  Skipping {model_name} - no best parameters found")
            continue
            
        print(f"\nğŸ”§ GridSearchCVë¡œ {model_name} ì •ë°€ ìµœì í™” ì‹œì‘...")
        model = MODELS[model_name]

        # ê° ëª¨ë¸ì— ë”°ë¥¸ íŒŒë¼ë¯¸í„° ê·¸ë¦¬ë“œ ì„¤ì •
        if model_name in ["RandomForest", "ExtraTrees"]:
            param_grid = {
                "max_depth": [
                    max(1, best_params[model_name]["max_depth"] - 2),
                    best_params[model_name]["max_depth"],
                    best_params[model_name]["max_depth"] + 2,
                ],
                "min_samples_split": [
                    max(2, best_params[model_name]["min_samples_split"] - 1),
                    best_params[model_name]["min_samples_split"],
                    best_params[model_name]["min_samples_split"] + 1,
                ],
                "min_samples_leaf": [
                    best_params[model_name]["min_samples_leaf"] - 1,
                    best_params[model_name]["min_samples_leaf"],
                    best_params[model_name]["min_samples_leaf"] + 1,
                ],
            }

        elif model_name == "XGBoost":
            param_grid = {
                "subsample": [
                    best_params[model_name]["subsample"] * 0.8,
                    best_params[model_name]["subsample"],
                    best_params[model_name]["subsample"] * 1.2,
                ]
            }

        elif model_name == "LightGBM":
            param_grid = {
                "num_leaves": [
                    max(10, best_params[model_name]["num_leaves"] - 10),
                    best_params[model_name]["num_leaves"],
                    best_params[model_name]["num_leaves"] + 10,
                ]
            }

        elif model_name == "CatBoost":
            param_grid = {
                "learning_rate": [
                    best_params[model_name]["learning_rate"] * 0.8,
                    best_params[model_name]["learning_rate"],
                    best_params[model_name]["learning_rate"] * 1.2,
                ]
            }

        elif model_name == "LogisticRegression":
            param_grid = {
                "C": [
                    best_params[model_name]["C"] * 0.8,
                    best_params[model_name]["C"],
                    best_params[model_name]["C"] * 1.2,
                ]
            }

        elif model_name == "SVC":
            param_grid = {
                "C": [
                    best_params[model_name]["C"] * 0.8,
                    best_params[model_name]["C"],
                    best_params[model_name]["C"] * 1.2,
                ]
            }

        elif model_name == "AdaBoost":
            param_grid = {
                "learning_rate": [
                    best_params[model_name]["learning_rate"] * 0.8,
                    best_params[model_name]["learning_rate"],
                    best_params[model_name]["learning_rate"] * 1.2,
                ]
            }

        elif model_name == "HistGradientBoosting":
            param_grid = {
                "max_depth": [
                    max(1, best_params[model_name]["max_depth"] - 2),
                    best_params[model_name]["max_depth"],
                    best_params[model_name]["max_depth"] + 2,
                ]
            }

        elif model_name == "GradientBoosting":
            param_grid = {
                "max_depth": [
                    max(1, best_params[model_name]["max_depth"] - 2),
                    best_params[model_name]["max_depth"],
                    best_params[model_name]["max_depth"] + 2,
                ]
            }

        elif model_name == "KNeighbors":
            param_grid = {
                "n_neighbors": [
                    max(1, best_params[model_name]["n_neighbors"] - 1),
                    best_params[model_name]["n_neighbors"],
                    best_params[model_name]["n_neighbors"] + 1,
                ]
            }

        elif model_name == "RidgeClassifier":
            param_grid = {
                "alpha": [
                    best_params[model_name]["alpha"] * 0.8,
                    best_params[model_name]["alpha"],
                    best_params[model_name]["alpha"] * 1.2,
                ]
            }

        elif model_name == "MLPClassifier":
            param_grid = {
                "alpha": [
                    best_params[model_name]["alpha"] * 0.8,
                    best_params[model_name]["alpha"],
                    best_params[model_name]["alpha"] * 1.2,
                ]
            }

        # GridSearchCV ì‹¤í–‰
        try:
            grid_search = GridSearchCV(
                estimator=model, param_grid=param_grid, cv=10, scoring="accuracy", n_jobs=-1
            )
            grid_search.fit(X_selected, y_smote)
            final_models[model_name] = grid_search.best_estimator_

            # ìµœì  í•˜ì´í¼íŒŒë¼ë¯¸í„° ë° êµì°¨ ê²€ì¦ ì •í™•ë„ ì¶œë ¥
            print(f" {model_name} GridSearchCV ì™„ë£Œ!")
            print(f" Best params for {model_name} after GridSearchCV: {grid_search.best_params_}")
            print(f" Best CV Accuracy for {model_name}: {grid_search.best_score_:.4f}")
        except Exception as e:
            print(f"  Error in GridSearchCV for {model_name}: {e}")
            continue

    return final_models

    # ë² ì´ìŠ¤ ëª¨ë¸ ì €ì¥
def __save_models(final_models):
    # ëª¨ë¸ë“¤ì„ ì €ì¥í•  ë””ë ‰í† ë¦¬ ì„¤ì •
    model_save_path = "./models/"
    
    # ë””ë ‰í† ë¦¬ê°€ ì—†ìœ¼ë©´ ìƒì„±
    import os
    os.makedirs(model_save_path, exist_ok=True)

    # ëª¨ë¸ ì €ì¥
    for model_name, model in final_models.items():
        try:
            file_path = f"{model_save_path}{model_name}.pkl"
            with open(file_path, "wb") as file:
                pickle.dump(model, file)
            print(f"Model {model_name} saved to {file_path}")
        except Exception as e:
            print(f" Error saving {model_name}: {e}")


@reset_seeds
def get_model(X_train=None, y_train=None, X_test=None, y_test=None, auto_load=True):
    """
    ìë™ìœ¼ë¡œ ë°ì´í„°ë¥¼ ì¸ì‹í•˜ê³  ëª¨ë¸ì„ í•™ìŠµí•˜ëŠ” í•¨ìˆ˜
    
    Parameters:
    X_train: í•™ìŠµ í”¼ì²˜ ë°ì´í„° (DataFrame)
    y_train: í•™ìŠµ íƒ€ê²Ÿ ë°ì´í„° (Series)
    X_test: í…ŒìŠ¤íŠ¸ í”¼ì²˜ ë°ì´í„° (DataFrame, ì„ íƒì‚¬í•­)
    y_test: í…ŒìŠ¤íŠ¸ íƒ€ê²Ÿ ë°ì´í„° (Series, ì„ íƒì‚¬í•­)
    auto_load: ë°ì´í„°ê°€ ì—†ì„ ë•Œ ìë™ ë¡œë“œ ì—¬ë¶€ (ê¸°ë³¸ê°’: True)
    
    Returns:
    final_models: í•™ìŠµëœ ëª¨ë¸ë“¤ì˜ ë”•ì…”ë„ˆë¦¬
    """
    # ë°ì´í„°ê°€ ì œê³µë˜ì§€ ì•Šì€ ê²½ìš° ìë™ìœ¼ë¡œ ë¡œë“œ
    if (X_train is None or y_train is None) and auto_load:
        print(" ë°ì´í„°ë¥¼ ìë™ìœ¼ë¡œ ë¡œë“œí•©ë‹ˆë‹¤...")
        try:
            from utilities.preprocess import get_preprocessed_data
            X_train, X_test, y_train = get_preprocessed_data()
            print(f" ë°ì´í„° ë¡œë“œ ì™„ë£Œ: X_train shape={X_train.shape}, y_train shape={y_train.shape}")
        except Exception as e:
            print(f"ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨: {e}")
            print("ì§ì ‘ ë°ì´í„°ë¥¼ ì „ë‹¬í•´ì£¼ì„¸ìš”.")
            return {}
    elif X_train is None or y_train is None:
        print("ë°ì´í„°ê°€ ì œê³µë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        print(" X_trainê³¼ y_trainì„ ì§ì ‘ ì „ë‹¬í•˜ê±°ë‚˜ auto_load=Trueë¡œ ì„¤ì •í•˜ì„¸ìš”.")
        return {}
    
    # ë°ì´í„° ê²€ì¦
    if X_train.shape[0] != y_train.shape[0]:
        raise ValueError(f"X_trainê³¼ y_trainì˜ ìƒ˜í”Œ ìˆ˜ê°€ ì¼ì¹˜í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: X_train={X_train.shape[0]}, y_train={y_train.shape[0]}")
    
    print(f"ëª¨ë¸ í•™ìŠµ ì‹œì‘...")
    print(f"í•™ìŠµ ë°ì´í„° í˜•íƒœ: {X_train.shape}")
    print(f"íƒ€ê²Ÿ ë°ì´í„° í˜•íƒœ: {y_train.shape}")
    
    # í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™” ë° ëª¨ë¸ í•™ìŠµ
    best_params = __run_prehpo(X_train, y_train)
    final_models = __run_hpo(best_params, X_train, y_train)
    __save_models(final_models)
    
    print(f" ëª¨ë¸ í•™ìŠµ ì™„ë£Œ! {len(final_models)}ê°œì˜ ëª¨ë¸ì´ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
    return final_models